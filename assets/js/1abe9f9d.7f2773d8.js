"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[652],{5839:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"reinforcement-learning/intro-rl","title":"Introduction to Reinforcement Learning","description":"Reinforcement Learning (RL) is the foundation of intelligent robot behavior. Unlike supervised learning, RL agents learn through trial and error, receiving rewards for good actions and penalties for poor ones.","source":"@site/docs/reinforcement-learning/intro-rl.md","sourceDirName":"reinforcement-learning","slug":"/reinforcement-learning/intro-rl","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/reinforcement-learning/intro-rl","draft":false,"unlisted":false,"editUrl":"https://github.com/KulsumShaikh16/Physical-AI-Humanoid-Robotics-Course/tree/main/my-website/docs/reinforcement-learning/intro-rl.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Deep Reinforcement Learning","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/reinforcement-learning/deep-rl"},"next":{"title":"Robot Simulation","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/category/robot-simulation"}}');var r=t(4848),o=t(8453);const a={sidebar_position:1},s="Introduction to Reinforcement Learning",l={},d=[{value:"The RL Framework",id:"the-rl-framework",level:2},{value:"Core Components",id:"core-components",level:3},{value:"The Agent-Environment Loop",id:"the-agent-environment-loop",level:3},{value:"A Simple Example: Robot Navigation",id:"a-simple-example-robot-navigation",level:2},{value:"Q-Learning Algorithm",id:"q-learning-algorithm",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Exploration vs. Exploitation",id:"exploration-vs-exploitation",level:3},{value:"The Bellman Equation",id:"the-bellman-equation",level:3},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"introduction-to-reinforcement-learning",children:"Introduction to Reinforcement Learning"})}),"\n",(0,r.jsx)(n.p,{children:"Reinforcement Learning (RL) is the foundation of intelligent robot behavior. Unlike supervised learning, RL agents learn through trial and error, receiving rewards for good actions and penalties for poor ones."}),"\n",(0,r.jsx)(n.h2,{id:"the-rl-framework",children:"The RL Framework"}),"\n",(0,r.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,r.jsx)(n.p,{children:"Every RL problem consists of:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Agent"})," \u2013 The learner/decision maker (your robot)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment"})," \u2013 The world the agent interacts with"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State"})," \u2013 Current situation of the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"})," \u2013 What the agent can do"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reward"})," \u2013 Feedback signal indicating how good an action was"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"the-agent-environment-loop",children:"The Agent-Environment Loop"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"State (s_t) \u2192 Agent \u2192 Action (a_t) \u2192 Environment \u2192 Reward (r_t) + State (s_{t+1})\n                \u2191                                              \u2193\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"a-simple-example-robot-navigation",children:"A Simple Example: Robot Navigation"}),"\n",(0,r.jsx)(n.p,{children:"Let's implement a basic grid world where a robot learns to navigate to a goal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass GridWorld:\n    def __init__(self, size=5):\n        self.size = size\n        self.agent_pos = [0, 0]\n        self.goal_pos = [size-1, size-1]\n    \n    def reset(self):\n        """Reset environment to initial state"""\n        self.agent_pos = [0, 0]\n        return self.get_state()\n    \n    def get_state(self):\n        """Return current state"""\n        return tuple(self.agent_pos)\n    \n    def step(self, action):\n        """\n        Take action and return (next_state, reward, done)\n        Actions: 0=up, 1=right, 2=down, 3=left\n        """\n        moves = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n        new_pos = [\n            self.agent_pos[0] + moves[action][0],\n            self.agent_pos[1] + moves[action][1]\n        ]\n        \n        # Check boundaries\n        if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size:\n            self.agent_pos = new_pos\n        \n        # Calculate reward\n        if self.agent_pos == self.goal_pos:\n            reward = 10.0\n            done = True\n        else:\n            reward = -0.1  # Small penalty for each step\n            done = False\n        \n        return self.get_state(), reward, done\n\n# Test the environment\nenv = GridWorld(size=5)\nstate = env.reset()\nprint(f"Initial state: {state}")\n\n# Take random actions\nfor _ in range(10):\n    action = np.random.randint(0, 4)\n    state, reward, done = env.step(action)\n    print(f"State: {state}, Reward: {reward}, Done: {done}")\n    if done:\n        break\n'})}),"\n",(0,r.jsx)(n.h2,{id:"q-learning-algorithm",children:"Q-Learning Algorithm"}),"\n",(0,r.jsx)(n.p,{children:"Q-Learning is a fundamental RL algorithm that learns the value of taking actions in states:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class QLearningAgent:\n    def __init__(self, state_size, action_size, learning_rate=0.1, \n                 discount_factor=0.95, epsilon=1.0):\n        self.q_table = np.zeros((state_size, state_size, action_size))\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n    \n    def get_action(self, state):\n        """Epsilon-greedy action selection"""\n        if np.random.random() < self.epsilon:\n            return np.random.randint(0, 4)  # Random action\n        else:\n            return np.argmax(self.q_table[state[0], state[1]])  # Best action\n    \n    def update(self, state, action, reward, next_state, done):\n        """Update Q-value using Bellman equation"""\n        current_q = self.q_table[state[0], state[1], action]\n        \n        if done:\n            target_q = reward\n        else:\n            max_next_q = np.max(self.q_table[next_state[0], next_state[1]])\n            target_q = reward + self.gamma * max_next_q\n        \n        # Q-learning update rule\n        self.q_table[state[0], state[1], action] += self.lr * (target_q - current_q)\n    \n    def decay_epsilon(self):\n        """Reduce exploration over time"""\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n# Train the agent\nenv = GridWorld(size=5)\nagent = QLearningAgent(state_size=5, action_size=4)\n\nepisodes = 1000\nfor episode in range(episodes):\n    state = env.reset()\n    total_reward = 0\n    \n    for step in range(100):\n        action = agent.get_action(state)\n        next_state, reward, done = env.step(action)\n        agent.update(state, action, reward, next_state, done)\n        \n        state = next_state\n        total_reward += reward\n        \n        if done:\n            break\n    \n    agent.decay_epsilon()\n    \n    if episode % 100 == 0:\n        print(f"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsx)(n.h3,{id:"exploration-vs-exploitation",children:"Exploration vs. Exploitation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exploration"})," \u2013 Trying new actions to discover better strategies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exploitation"})," \u2013 Using known good actions to maximize reward"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Epsilon-Greedy"})," \u2013 Balance between random (explore) and best (exploit) actions"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"the-bellman-equation",children:"The Bellman Equation"}),"\n",(0,r.jsx)(n.p,{children:"The core of Q-learning:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Q(s, a) \u2190 Q(s, a) + \u03b1[r + \u03b3 max Q(s', a') - Q(s, a)]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u03b1 = learning rate"}),"\n",(0,r.jsx)(n.li,{children:"\u03b3 = discount factor"}),"\n",(0,r.jsx)(n.li,{children:"r = reward"}),"\n",(0,r.jsx)(n.li,{children:"s = current state, s' = next state"}),"\n",(0,r.jsx)(n.li,{children:"a = action"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,r.jsx)(n.admonition,{title:"Exercise 1: Tune Hyperparameters",type:"tip",children:(0,r.jsx)(n.p,{children:"Experiment with different learning rates, discount factors, and epsilon decay rates. How do they affect learning speed?"})}),"\n",(0,r.jsx)(n.admonition,{title:"Exercise 2: Add Obstacles",type:"tip",children:(0,r.jsx)(n.p,{children:"Modify the GridWorld to include obstacles. The agent should learn to navigate around them!"})}),"\n",(0,r.jsx)(n.admonition,{title:"Exercise 3: Visualize Learning",type:"tip",children:(0,r.jsx)(n.p,{children:"Create a visualization showing how the Q-values evolve during training."})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["Now that you understand basic RL, let's explore ",(0,r.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Course/docs/reinforcement-learning/deep-rl",children:"deep reinforcement learning"})," where we use neural networks instead of Q-tables!"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var i=t(6540);const r={},o=i.createContext(r);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);